---
layout: post

title:  "Elasticsearch 데이터 추출 및 전처리"

categories : Python

tag : [python, elasticsearch, json]

---





엘라스틱서치 접속 후 데이터 가져오기

그 후 데이터 전처리 작업 정리



```python
###시작 시간 입력
from datetime import datetime, timedelta

start_time = datetime(2024, 9, 1, 0, 0)
end_time = datetime(2024, 10, 1, 0, 0)
```

```python
import re
import json
import time
import pprint

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from elasticsearch import Elasticsearch

import functools as f
from operator import add



ES_DB_COLUMNS = {
    ### "필드 이름" : "어쩌고"
    "@timestamp": "timestamp_kst",
    "aaa": "a",
    "bb": "b",
    "ccc": "c"
}

ES_SEARCH_SIZE = 10000
ES_HOST = 'http://123.456.789:0000' #형태 = 주소:포트
ES_CONFIG = {'id': 'id1234', 'pwd': 'pw1234'}

def get_es(es_host=ES_HOST, es_config=ES_CONFIG):
    try:
        es = Elasticsearch(es_host, basic_auth=(es_config['id'], es_config['pwd']))
    except:
        raise Exception('ES Connection failed')
    return es
    
def get_log(es: Elasticsearch, query, source_includes):
    try:
        response = es.search(body=query, size=ES_SEARCH_SIZE, _source=True, _source_includes=source_includes)
    except:
        raise Exception('Failed to get response')
    return response['hits']['hits']

```

```python
def __flatten_meta(sp_meta):
    # extract information to classify category of logs and flatten informations.
    sp_result = {}
    # get values 
    sp_meta_value = sp_meta['value'] # 
    
    # extract level one information
    for sp_val in ['a', 'b']:
        if sp_val in sp_meta_value.keys():
            sp_result[sp_val] = sp_meta_value[sp_val] #
        else:
            sp_result[sp_val] = None
            
    # extract level two information            
    for sp_key, sp_val in [('c', 'd'), ('e', 'f'), ('g', 'h')]:
        if sp_key in sp_meta_value.keys() : 
            if sp_val in sp_meta_value[sp_key].keys():
                sp_result[sp_val] = sp_meta_value[sp_key][sp_val]
            else:
                sp_result[sp_val] = np.nan
        else:
            sp_result[sp_val] = np.nan                 
            
    # extract level two information            
    for sp_key, sp_sub_key, sp_val in [('c', 'd'), ('e', 'f'), ('g', 'h')]:
        if sp_key in sp_meta_value.keys():
            if sp_sub_key in sp_meta_value[sp_key].keys():
                sp_sub_value = sp_meta_value[sp_key]
                if sp_val in sp_sub_value[sp_sub_key].keys():
                    sp_result[f'{sp_sub_key}.{sp_val}'] = sp_sub_value[sp_sub_key][sp_val]
                else:
                    sp_result[f'{sp_sub_key}.{sp_val}'] = np.nan     
            else:
                sp_result[f'{sp_sub_key}.{sp_val}'] = np.nan
        else:
            sp_result[f'{sp_sub_key}.{sp_val}'] = np.nan
                    
    # rename fields 
    for sp_prev, sp_revi in [('c', 'd'), ('e', 'f'), ('g', 'h')]:
        if sp_prev in sp_result.keys():
            sp_prev_data = sp_result.pop(sp_prev)
            sp_result[sp_revi] = sp_prev_data
    return sp_result 
        

        
def __flatten_dict(sp_dict):
    # flatten dictionary data (extract target files within level 3)
    
    result = {}     
    for sp_key, sp_val in sp_dict.items():
        # preprocess string type dictionary
        if sp_key == 'aaa':
            sp_val = re.sub(r'\bnull\b', 'None', sp_val)
            sp_val = eval(sp_val)
        
        if isinstance(sp_val, dict):
            if sp_key == 'aaa' : 
                result['aaa'] = sp_val
                
            elif sp_key == 'dist' :
                # extract two level informations
                for sub_key, sub_val in sp_val.items():
                    if sub_key == 'context': 
                        result['데이터'] = sp_val[sub_key]['데이터']
                    elif sub_key == 'value':
                        # there is no model information in some action logs 
                        if 'pipeline' in sp_val[sub_key]:
                            result['데이터'] = sp_val[sub_key]['pipeline']['데이터']
                        else:                             
                            result['데이터'] = None
                            
                        result['데이터'] = sp_val[sub_key]['inference']['데이터']
                        result['데이터'] = sp_val[sub_key]['데이터']
                    else:
                        result[sub_key] = sp_val[sub_key]
        else:
            result[sp_key] = sp_val
                            
    return result

def __response2df(resp):
    # transform respons into dataframe
            
    # flat response data (extract deep lv fiels into level 1)
    flat_resp = []
    for sp_idx, res in enumerate(resp):
        flat_resp.append(__flatten_dict(res['_source']))
    # warp as single dataframe             
    df = pd.DataFrame([dict(v) for v in flat_resp])
    # timestamp UTC -> KST         
    df['@timestamp'] = pd.to_datetime(df['@timestamp'], format="%Y-%m-%dT%H:%M:%S.%fZ")
    df['@timestamp'] += pd.Timedelta(hours=9) 
    
    # dictionary -> json string (+ remove emoji)
    df['aaa'] = df['aaa'].apply(lambda x: json.dumps(x, ensure_ascii=False))
    df['aaa'] = df['aaa'].apply(lambda x: json.dumps(x, ensure_ascii=False))
    # handle emtpy fields
    df = df.fillna('')
                        
    return df.rename(columns=ES_DB_COLUMNS) 

def __user_action_logs_to_df(logs):
    df_log = __response2df(logs)
    if df_log.empty :
        return pd.DataFrame([])
    else:
        return df_log
    
    
def __timerange2gtelte(time_range, timezone=9):
    # transform datetime into gte / lte w DSL format         
    if isinstance(time_range[0], datetime):
        #gte = datetime.strftime(time_range[0], "%Y-%m-%dT%H:%M:%S.%fZ")
        gte = datetime.strftime(time_range[0]-timedelta(hours=timezone), "%Y-%m-%dT%H:%M:%S.%fZ")
    else:
        # input is string
        #gte = datetime.strftime(datetime.strptime(time_range[0], "%Y-%m-%d %H:%M:%S"), "%Y-%m-%dT%H:%M:%S.%fZ")
        gte = datetime.strftime(datetime.strptime(time_range[0], "%Y-%m-%d %H:%M:%S")-timedelta(hours=timezone), "%Y-%m-%dT%H:%M:%S.%fZ")
    if isinstance(time_range[0], datetime):
        #lte = datetime.strftime(time_range[1], "%Y-%m-%dT%H:%M:%S.%fZ")
        lte = datetime.strftime(time_range[1]-timedelta(hours=timezone), "%Y-%m-%dT%H:%M:%S.%fZ")
    else:
        #lte = datetime.strftime(datetime.strptime(time_range[1], "%Y-%m-%d %H:%M:%S"), "%Y-%m-%dT%H:%M:%S.%fZ")
        lte = datetime.strftime(datetime.strptime(time_range[1], "%Y-%m-%d %H:%M:%S")-timedelta(hours=timezone), "%Y-%m-%dT%H:%M:%S.%fZ")
    
    return gte, lte



def get_user_action_logs(es:Elasticsearch, time_range):
    gte, lte = __timerange2gtelte(time_range)

    query = {
        "query": {
            "bool": {
                "must": [
                    {"exists": {"field": "필드명"}},
                    {"match_phrase": {"필드명"}}
                ],
                "filter": [
                    {"range": {"@timestamp": {
                    "format": "strict_date_optional_time",
                    "gte": gte,
                    "lte": lte}}
                    }
                ]
            }
        }
    } #
    
    # set fields to gather
    source_includes = [
        "필드명1", 
        "필드명2" 
    ]
    
    df = __user_action_logs_to_df(get_log(es, query, source_includes))

    return df 

def create_user_info(logs):
    user_data = {}  # 1번 로그들을 저장할 딕셔너리 (key: mail, value: list of userID)
    records = []

    # 1번 로그들 먼저 처리
    for log in logs:
        dist_data = log.get('_source', {}).get('dist', {})

        # 1번 로그: userID와 mail이 있는 경우
        if 'userID' in dist_data and 'mail' in dist_data:
            user_id = dist_data.get('userID')
            mail = dist_data.get('mail')
            if mail not in user_data:
                user_data[mail] = []
            user_data[mail].append(user_id)

    # 2번 로그들을 처리
    for log in logs:
        dist_data = log.get('_source', {}).get('dist', {})

        # 2번 로그: account 내 name과 username이 있는 경우
        if 'account' in dist_data:
            account_data = dist_data.get('account', {})
            username = account_data.get('username')
            name_info = account_data.get('name', "")

            # 이름과 부서를 분리
            if "@abc.com" not in username:
                try:
                    name, dept = name_info.split(" ", 1)
                except:
                    name = name_info
                    dept = ""
            else:
                name = name_info
                dept = "abc"

            # 1번 로그의 mail과 2번 로그의 username이 일치하는지 확인
            if username in user_data:
                for user_id in user_data[username]:
                    records.append({
                        'user_id': user_id,
                        'mail': username,
                        'name': name,
                        'dept': dept
                    })

    # DataFrame 생성
    df = pd.DataFrame(records)

    # userID를 기준으로 유니크한 값만 유지
    df = df.drop_duplicates(subset='user_id')

    return df



def get_user_info_logs(es:Elasticsearch, time_range):
    gte, lte = __timerange2gtelte(time_range)

    query = {
        "query": {
            "bool": {
                "must": [
                   {
                       "bool": {
                            "should": [
                                {"match_phrase": {"필드1": "abcd"}},
                                {"match_phrase": {"필드2": "abcdef"}},
                            ],
                            "minimum_should_match": 1
                        }
                    },
                    {
                        "bool": {
                            "should": [
                                {"match_phrase": {"message": "어쩌고"}},
                                {"match_phrase": {"message": "어쩌고2"}},
                                {"match_phrase": {"message":"어쩌고3"}}
                            ],
                            "minimum_should_match": 1
                       }
                    },
                ],
                "filter": [
                    {"range": {"@timestamp": {
                    "format": "strict_date_optional_time",
                    "gte": gte,
                    "lte": lte}}
                    }
                ]
            }
        }
    } #

    source_includes = [
        "필드1",
        "필드2",
        "필드3"
    ]

    logs = get_log(es, query, source_includes)
    return logs


def merge_user_data(user_action, user_info):
    df = pd.merge(user_action, user_info, on="user_id", how="inner")
    return df


def get_service_logs(es: Elasticsearch, start_time, end_time, save=True):
    time_range = (start_time, end_time)

    user_action = get_user_action_logs(es, time_range)
    user_info = create_user_info(get_user_info_logs(es, time_range))

    service_logs = merge_user_data(user_action, user_info)

#     if save:
#         filename = "{}_{}.xlsx".format(start_time.strftime("%Y%m%d"), end_time.strftime("%Y%m%d"))
#         service_logs.to_excel("{}".format(filename))

    return service_logs


### 유저 액션 2
def get_action_logs2(es:Elasticsearch, time_range): 
    gte, lte = __timerange2gtelte(time_range)

    query = {
        "query": {
            "bool": {
                "must": [
                    {
                        "bool": {
                            "should": [
                                {"match_phrase": {"message":"어쩌고"}},
                                {"match_phrase": {"message":"어쩌고2"}},
                                {"match_phrase": {"message": "어쩌고3"}}
                            ],
                            "minimum_should_match": 1
                       }
                    },
                ],
                "filter": [
                    {"range": {"@timestamp": {
                    "format": "strict_date_optional_time",
                    "gte": gte,
                    "lte": lte}}
                    }
                ]
            }
        }
    } #

    source_includes = [
        "ㅁㅁㅁ",
        "ㅇㅇㅇ",
        "ㅈㅈㅈ"
    ]

    logs_2 = get_log(es, query, source_includes)
    
    df_2=pd.DataFrame()
    
    for i in range(len(logs_2)):
        json_data=pd.json_normalize(logs_2[i])
        df_2=pd.concat([df_2,json_data], axis=0, ignore_index=True)
        df_2 = df_2[['_id','_source.@timestamp','_source.message']]

    df_2.columns=['id','timestamp','message']
    df_2['timestamp'] = pd.to_datetime(df_2['timestamp'], format="%Y-%m-%dT%H:%M:%S.%fZ")
    df_2['timestamp'] += pd.Timedelta(hours=9)
    
    df_2['message']=df_2['message'].apply(str_to_dict)
    df_2['a'] = df_2['message'].apply(lambda x : x.get('a'))
    df_2['message_text']=df_2['message'].apply(lambda x : x.get('message'))

    
    df_2['a']=df_2['a'].apply(str_to_dict)
    
    user_id_list=[]
    prompt_list=[]
    category_list=[]

    for i in range(len(df_2)):
        user_id= df_2['a'][i].get('userId')
        user_id_list.append(user_id)

        prompt = df_2['a'][i].get('prompt')
        prompt_list.append(prompt)

        category=df_2['a'][i].get('category')
        category_list.append(category)


    df_2['user_id']=user_id_list
    df_2['prompt'] = prompt_list
    df_2['category']=category_list

    df_2['type']='A'
    df_2['sub_type']=''
    df_2.loc[df_2['category'].notna(), 'sub_type'] = df_2['category']
    df_2.loc[df_2['message_text']=='텍스트 내용','sub_type']='a'
    df_2.loc[df_2['message_text']=='텍스트 내용2','sub_type']='b'

    return df_2


import ast
def str_to_dict(s):
    try:
        return ast.literal_eval(s)
    except (ValueError, SyntaxError):
        return {}

```

```python
if __name__ == "__main__":
    logs = get_service_logs(get_es(), start_time, end_time)
    logs_2= get_service_logs2(get_es(), start_time, end_time)
```

.

.

```python
## 특정 부서는 다 제외하기
without_a = df_logs[df_logs['dept']!="a"].reset_index(drop=True)

## 특정 메일만 제외하기
without_a = without_a[~without_a['mail'].isin(['a@abc.com','b@abc.com','c@abc.com'])].reset_index(drop=True)
```

.

.

```python
## 특정 기능만 사용했을 시, 그 기능만 피벗 테이블로 만들기
unique_values = total_action['type'].unique().tolist()

count_all=total_action.groupby(['date','type']).agg({'timestamp':'count'}).reset_index()
count_all.columns=['date','type','사용량']
pv_count_all = pd.pivot_table(count_all, index='date', columns='type', values='사용량',aggfunc='sum')
pv_count_all = pv_count_all[unique_values]
```

.

.

```python
def __assign_type(row):
    # assign action category with preprocessed row data         
    if str(row['zzz']).startswith("[["):
        return "z"
    elif str(row['m'])=='mmm':
        return "AA"
    elif pd.notna(row['asadasd']) or pd.notna(row['asd']):
        return "BB"
    elif pd.notna(row['asddsadwww']) and pd.notna(row['adsd']):
        return "asda"
    else :
        return "기타"

df_logs['type'] = df_logs.apply(__assign_type, axis=1)

## 타입이 AA일 경우 a 컬럼을 만들고 각 행별로 값을 분류, 타입이 BB일 경우 컬럼 b를 만들고 분류
Aa = df_logs['type'] == 'AA'
Bb = df_logs['type'] == 'BB'

df_logs.loc[Aa, 'a'] = df_logs['abcd'].apply(lambda x: "o" if str(x).startswith("특정 단어") else "x")
df_logs.loc[Bb, 'b'] = df_logs['efg'].apply(lambda x: "분류1" if str(x).startswith("특정 단어") else "")
```

