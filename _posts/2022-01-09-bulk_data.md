---
layout: post

title:  "대용량 데이터 합산"

categories : Python

tag : [python, pandas]
---





데이터 한 줄마다 log_id가 각각 붙어 있고 team_id별로 몇 개의 log_id가 있는지 구하려는 게 목표



동일한 종류의 데이터인데 1억 줄이 넘어서 메모리 에러가 발생했었음

처음에 데이터 결과를 SQL로 빼내서 저장할 때 1.txt, 2.txt... 5개를 만들어둠



**성공 코드**

```python
#반복문으로 처리하여 성공한 코드
import pandas as pd
  
df = pd.DataFrame()
for i in range(6):
    df1 = pd.read_csv(str(i+1)+".txt", sep = "\t")
    df1 = df1.groupby('team_id').agg({'tp_acquire_log_id': 'count'})
    df= df.append(df1)
 
# team_id 별로 여러 개의 값이 있어서 다시 더해주기
df = df.groupby('team_id').agg({'tp_acquire_log_id':'sum'})
 
# 내림차순 정렬
df = df.sort_values(by='tp_acquire_log_id', ascending = False )
```

<img src="../../../../img/2022-01-09-bulk_data/data.png" alt="data" style="zoom: 80%;" />



---



**처음에 실패했던 코드**

- 총 1.5억 줄 정도 되다 보니 group by, count에서 에러 발생

```python
import pandas as pd
 
df1 = pd.read_csv('1.txt', sep = '\t')
df2 = pd.read_csv('1.txt', sep = '\t')
df3 = pd.read_csv('1.txt', sep = '\t')
df4 = pd.read_csv('1.txt', sep = '\t')
df5 = pd.read_csv('1.txt', sep = '\t')


df6 = df1.append([df2,df3,df4,df5], ignore_index = True)
df7 = df6.groupby('team_id')['tp_acquire_log_id'].count() #여기서 메모리 에러 발생 
```



---



**두 번째 실패했던 코드**

- 각각 group by 후에 더하기 시도
  - team_id 들이 df1_1 ~ df5_1에 공통적으로 있지 않으면 NaN 이 뜨면서 에러 발생

```python
import pandas as pd
 
df1 = pd.read_csv('1.txt', sep = '\t')
df2 = pd.read_csv('1.txt', sep = '\t')
df3 = pd.read_csv('1.txt', sep = '\t')
df4 = pd.read_csv('1.txt', sep = '\t')
df5 = pd.read_csv('1.txt', sep = '\t')


df1_1 = df1.groupby('team_id')['tp_acquire_log_id'].count()
df2_1 = df2.groupby('team_id')['tp_acquire_log_id'].count()
df3_1 = df3.groupby('team_id')['tp_acquire_log_id'].count()
df4_1 = df4.groupby('team_id')['tp_acquire_log_id'].count()
df5_1 = df5.groupby('team_id')['tp_acquire_log_id'].count()
 
 
df6 = df1_1 + df2_1 + df3_1 + df4_1 + df5_1
dataframe = pd.DataFrame(df6)
df7 = dataframe.sort_values(by='tp_acquire_log_id', ascending = False)
```