---
layout: post

title:  "혼자 공부하는 머신러닝 06"
categories : Machine_Learning

tag : [python, ml,머신러닝, sklearn, 앙상블, 그레이디언트부스팅, 랜덤포레스트, 부트스트랩]

---

### 0. 앙상블 학습

앙상블이란? 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘

.

.

### 1. 랜덤 포레스트

```python
#데이터 준비
import pandas as pd
import numpy as np

wine = pd.read_csv('https://bit.ly/wine-date')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42
)
```



```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=-1, random_state=42)  
scores=cross_validate(rf, train_input, train_target, 
                      return_train_score=True, n_jobs=-1) 

#n_jobs -1로 두면 모든 CPU 코어 사용
#return_train_score True로 두면 검증 점수 뿐만 아니라 훈련 세트 점수도 반환

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
#0.9973541965122431 0.8905151032797809
## 훈련세트에 다소 과대적합

rf.fit(train_input, train_target)
print(rf.feature_importances_)

[0.23167441 0.50039841 0.26792718]
```

랜덤 포레스트는 대표적인 결정 트리 기반의 앙상블 학습 방법.

부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징.

부트스트랩이란, 데이터 세트에서 중복을 허용하여 데이터를 샘플링 하는 방식.

.

```python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```

랜덤 포레스트는 훈련 세트에서 중복을 허용하여 부트스트랩 샘플 만들어서 결정 트리를 훈련

부트스트랩 샘플에 포함되지 않고 남는 샘플이 있는데 이를 OOB(out of Bag) 샘플

이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다. 마치 검증 세트 역할.

.

.

### 2. 엑스트라 트리

```python
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state =42)
scores = cross_validate(et, train_input, train_target,
                        return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

et.fit(train_input, train_target)
#0.9974503966084433 0.8887848893166506
print(et.feature_importances_)
[0.20183568 0.52242907 0.27573525]
```

엑스트라 트리는 랜덤 포레스트와 비슷하게 결정 트리를 사용하여 앙상블 모델을 만들지만

부트스트랩 샘플을 사용하지는 않음.

대신 랜덤하게 노드를 분할해 과대적합을 감소시킴.

.

.

### 3. 그레이디언트 부스팅 / 히스토그램 기반 그레이디언트 부스팅

랜덤 포레스트나 엑스트라 트리와 달리 결정 트리를 연속적으로 추가하여 손실함수를 최소화하는 앙상블 방법.

훈련 속도가 조금 느리지만 더 좋은 성능을 기대할 수 있음.



그레이디언트 부스팅의 속도를 개선한 것이 히스토그램 기반 그레이디언트 부스팅.

안정적인 결과와 높은 성능으로 인기가 높음.

```python
## 그레이디언트 부스팅

from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)
scores=cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
#0.8881086892152563 0.8720430147331015

# 결정 트리 개수를 500개로 5배 늘리고 learing_rate도 0.1에서 0.2로 늘리면
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores= cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
#0.9464595437171814 0.8780082549788999
# 많이 늘렸지만 과대 적합을 꽤나 억제

gb.fit(train_input, train_target)
print(gb.feature_importances_)

#랜덤 포레스트 때보다 sugar에 좀 더 집중
# 3개 각각 'alcohol', 'sugar', 'pH'
[0.15882696 0.6799705  0.16120254]
```

.

.

```python
# 히스토그램 기반 그레이디언트 부스팅

from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier

hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target,
                        return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

```python
# Pernutation Importance
from sklearn.inspection import permutation_importance

hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10,
                               random_state=42, n_jobs=-1)

print(result.importances_mean)


result = permutation_importance(hgb, test_input, test_target, n_repeats=10,
                               random_state=42, n_jobs=-1)

print(result.importances_mean)
hgb.score(test_input, test_target)

[0.08876275 0.23438522 0.08027708]
[0.05969231 0.20238462 0.049     ]
0.8723076923076923
```

```python
from lightgbm import LGBMClassifier

lgb = LGBMClassifier(random_state=42)
scores=cross_validate(lgb, train_input, train_target,
                     return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

